{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNguzZY8i65yqHf4sLGk+hh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nicks165/Analysis-Tensorflow/blob/master/Deeplake_Vectorstore.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip3 install deeplake\n",
        "!pip install \"deeplake[enterprise]\"\n",
        "!pip3 install cohere datasets\n",
        "\n",
        "import os\n",
        "from deeplake.core.vectorstore.deeplake_vectorstore import VectorStore\n",
        "import numpy as np\n",
        "\n",
        "# global variables\n",
        "dataset_path = \"hub://test_anx/wiki_articles_v2\"\n",
        "token = 'eyJhbGciOiJIUzUxMiIsImlhdCI6MTY4NzY3NzQ3MSwiZXhwIjoxNjg4OTczNDE5fQ.eyJpZCI6Im5pY2tzMTY1In0.ExPskNRBIEWIsr3lSoEUKfq7-esb49NAQP0D08J4cJreBSnbsx1ZM2FsLpEsVNXtYbF_KtRu9JYvd3rASfldeg'"
      ],
      "metadata": {
        "id": "5iYuO0L79KOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a vector store or load if it already exists based on the dataset_path\n",
        "def create_load_table():\n",
        "  uri = dataset_path\n",
        "\n",
        "  vector_store = VectorStore(\n",
        "    path = dataset_path,\n",
        "#    runtime = {\"tensor_db\": True}, ## Failed\n",
        "#    runtime = { \"db_engine\" : True}, ## Failed\n",
        "    token = token,\n",
        "    )\n",
        "\n",
        "  return vector_store"
      ],
      "metadata": {
        "id": "D2fAqkIGTDLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.utils.text import string\n",
        "import time\n",
        "\n",
        "# Add one record to the table. measures transactional latency at different DB sizes\n",
        "def upsert_one_record(vector_store, co):\n",
        "\n",
        "  # add 1 more record\n",
        "  text = \"How are you WORLD?\"\n",
        "  embedding = co.embed(texts=[text], model='multilingual-22-12').embeddings[0]\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  vector_store.add(text = [text],\n",
        "                 embedding = [embedding],\n",
        "                 metadata = [{\"views\" : 600.00}])\n",
        "\n",
        "  print(\"Updated with one Record and Time taken --- %s seconds ---\" % (time.time() - start_time))\n"
      ],
      "metadata": {
        "id": "vFD_MnkqASDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# couple vector search with filtering based on metadata. In this case we filtr based on number of views.\n",
        "# tried carrying out both Python (client based) and TQL based search but TQL based search fails.\n",
        "def conditional_search(vector_store, co):\n",
        "\n",
        "  query1 = \"What was the cause of the major recession in the early 20th century?\"\n",
        "  query2 = \"Where is Mount Everest?\"\n",
        "  query3 = \"something else\"\n",
        "\n",
        "  queries = [query1, query2, query3]\n",
        "  near_vectors = co.embed(texts=queries, model='multilingual-22-12').embeddings\n",
        "\n",
        "  timeTakenList = []\n",
        "  timeTakenListTensorQuery = []\n",
        "\n",
        "  ## Issue 3 queries and take the average\n",
        "  for i in range(0, 2):\n",
        "    query_start_time = time.time()\n",
        "\n",
        "    # Add a filter to your search\n",
        "    data = vector_store.search(\n",
        "       embedding = near_vectors[i],\n",
        "       exec_option = \"python\",\n",
        "       filter = {\"metadata\": {\"views\": 600}}, # Only valid for exec_option = \"python\"\n",
        "       k = 2,\n",
        "    )\n",
        "\n",
        "    query_end_time = time.time()\n",
        "    timeTakenList.append(query_end_time - query_start_time)\n",
        "\n",
        "    # Search using TQL\n",
        "    # Format the embedding array or list as a string, so it can be passed in the REST API request.\n",
        "    #query_start_time = time.time()\n",
        "\n",
        "    #embedding_string = \",\".join([str(item) for item in near_vectors[i]])\n",
        "\n",
        "    #tql_query = f\"select * from (select text, cosine_similarity(embedding, ARRAY[{embedding_string}]) as score) where 'metadata/views' > 300 order by score desc limit 2\"\n",
        "\n",
        "    #data = vector_store.search(\n",
        "    #   query = tql_query,\n",
        "    #   exec_option = \"tensor_db\", # Only valid for exec_option = \"compute_engine\" or \"tensor_db\"\n",
        "    #)\n",
        "\n",
        "    #query_end_time = time.time()\n",
        "\n",
        "    #timeTakenListTensorQuery.append(query_end_time - query_start_time)\n",
        "\n",
        "    #print(\"For query number {0}, time taken for conditional search = {1} \".format(i+1, timeTakenList[i]))\n",
        "\n",
        "  averageTimeTaken = numpy.average(timeTakenList)\n",
        "  #averageTimeTakenTQL = numpy.average(timeTakenListTensorQuery)\n",
        "  print(\"Average time taken for conditional search using Python exec option = {0} \".format(averageTimeTaken))\n",
        "  #print(\"Average time taken for conditional search using TQL and tensor_db exec option = {0} \".format(averageTimeTaken))"
      ],
      "metadata": {
        "id": "eKJK5bMlAPG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "\n",
        "# measure vector search latency wihtout any additional filtering\n",
        "# tried carrying out both Python (client based) and TQL based search but TQL based search fails.\n",
        "def issue_measure_query_time(vector_store, co):\n",
        "\n",
        "  query1 = \"What was the cause of the major recession in the early 20th century?\"\n",
        "  query2 = \"Where is Mount Everest?\"\n",
        "  query3 = \"something else\"\n",
        "\n",
        "  queries = [query1, query2, query3]\n",
        "  near_vectors = co.embed(texts=queries, model='multilingual-22-12').embeddings\n",
        "\n",
        "  timeTakenList = []\n",
        "  timeTakenListTensorQuery = []\n",
        "\n",
        "  ## Issue 3 queries and take the average\n",
        "  for i in range(0, 2):\n",
        "    query_start_time = time.time()\n",
        "\n",
        "    #search_results = vector_store.search(embedding = np.random.random(768).astype('float32'))\n",
        "\n",
        "    # simple vector search\n",
        "    data = vector_store.search(\n",
        "       embedding = near_vectors[i],\n",
        "       k = 2,\n",
        "    )\n",
        "\n",
        "    query_end_time = time.time()\n",
        "\n",
        "    timeTakenList.append(query_end_time - query_start_time)\n",
        "\n",
        "    # Search using TQL\n",
        "    # Format the embedding array or list as a string, so it can be passed in the REST API request.\n",
        "    #query_start_time = time.time()\n",
        "\n",
        "    #embedding_string = \",\".join([str(item) for item in near_vectors[i]])\n",
        "\n",
        "    #tql_query = f\"select * from (select text, cosine_similarity(embedding, ARRAY[{embedding_string}]) as score) order by score desc limit 2\"\n",
        "\n",
        "    #data = vector_store.search(\n",
        "    #   query = tql_query,\n",
        "    #   exec_option = \"tensor_db\", # Only valid for exec_option = \"compute_engine\" or \"tensor_db\"\n",
        "    #)\n",
        "\n",
        "    #query_end_time = time.time()\n",
        "\n",
        "    #timeTakenListTensorQuery.append(query_end_time - query_start_time)\n",
        "\n",
        "\n",
        "  averageTimeTaken = numpy.average(timeTakenList)\n",
        "  #averageTimeTakenTQL = numpy.average(timeTakenListTensorQuery)\n",
        "  print(\"Average time taken for vector search using Python exec option = {0} \".format(averageTimeTaken))\n",
        "  #print(\"Average time taken for vector search using TQL and tensor_db exec option = {0} \".format(averageTimeTaken))\n",
        "  #print(\"Average time taken for vector search = {0} \".format(averageTimeTaken))"
      ],
      "metadata": {
        "id": "CSQbmp_TALUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the main execution function\n",
        "\n",
        "import time\n",
        "import string\n",
        "import random\n",
        "import cohere\n",
        "import numpy\n",
        "\n",
        "MAX_ENTRIES = 35000000\n",
        "\n",
        "# Inserts batches in the database according to the chunk size\n",
        "# On steps of 100K inserts, kick off measurements for search and single insert\n",
        "# Retry a batch thrice before moving on\n",
        "def upsert_db_measure(chunk, metadata_list, vector_store, batch_size, total_inserted, workload_start_time, co, previous_run_time):\n",
        "\n",
        "  # catch and retry and maintain count of number of retries and time taken for each\n",
        "  retries = 0\n",
        "  error = \"\"\n",
        "  while True:\n",
        "    if retries > 2:\n",
        "      print (\"Abondon batch after 3 retries to insert data. error = {0}\".format(error))\n",
        "      return total_inserted\n",
        "    else:\n",
        "      try:\n",
        "        #payload_to_insert.append({\"text\" : chunk[\"text\"][i], \"vector\" : chunk['emb'][i], \"views\" : {\"views\": chunk['views'][i]}})\n",
        "        data = vector_store.add(\n",
        "            text = chunk[\"text\"],\n",
        "            embedding = chunk[\"emb\"],\n",
        "            metadata = metadata_list)\n",
        "\n",
        "        total_inserted += batch_size\n",
        "        break # break out of the infinite while loop\n",
        "      except Exception as err:\n",
        "        retries += 1\n",
        "        error = err\n",
        "        continue\n",
        "\n",
        "  if(total_inserted in range(0, MAX_ENTRIES, 100000)):\n",
        "    print(\"=======================================================================================================\")\n",
        "\n",
        "    total_time = (time.time() - workload_start_time) + previous_run_time\n",
        "    print(\"For {0} entries, time taken for inserts = {1} \".format(total_inserted, total_time))\n",
        "\n",
        "    # now get data for 1 single insert\n",
        "    upsert_one_record(vector_store, co)\n",
        "    issue_measure_query_time(vector_store, co)\n",
        "    conditional_search(vector_store, co)\n",
        "\n",
        "  return total_inserted"
      ],
      "metadata": {
        "id": "D791-97A-gA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import cohere\n",
        "\n",
        "# Create a generator that yields chunks of the dataset\n",
        "def chunk_generator(dataset, chunk_size, starting_index):\n",
        "  for i in range(starting_index, len(dataset), chunk_size):\n",
        "    yield dataset[i:i + chunk_size]\n",
        "\n",
        "\n",
        "def load_cohere_dataset():\n",
        "   # bring dataset to disk in Arrow table format\n",
        "  dataset = load_dataset(f\"Cohere/wikipedia-22-12-en-embeddings\", split=\"train\")\n",
        "  return dataset\n",
        "\n",
        "# The main executable and orchestrator function\n",
        "def load_execute_workload(dataset):\n",
        "\n",
        "  limit = -1 # keep -1 for all, else update to a positive number to limit\n",
        "  chunk_size = 1000 # size of batch upserts and items kept in memory\n",
        "  #if the runs fails, we want to re-start and for the subsequent time measurements to be valid. Use this variable\n",
        "  # paste the runtime for previous starting_index or set it to 0 otherwise\n",
        "  previous_run_time = 0 # set to 0 when starting from scratch\n",
        "  previous_docs_loaded = 0 # set to 0 when starting from scratch\n",
        "  max_docs_loaded = previous_docs_loaded\n",
        "\n",
        "  start_time = time.time()\n",
        "\n",
        "  metadata_list = []\n",
        "\n",
        "  vector_store = create_load_table()\n",
        "\n",
        "  co = cohere.Client(f\"o7lTEJeC1QHjU5I4Ee6U2I0m6l5wCOUPWqwoGM7H\")  # Add your cohere API key from www.cohere.com\n",
        "\n",
        "  # Iterate over the chunks\n",
        "  for chunk in chunk_generator(dataset, chunk_size, previous_docs_loaded):\n",
        "    metadata_list = [{\"views\" : value} for value in chunk['views']]\n",
        "    max_docs_loaded = upsert_db_measure(chunk, metadata_list, vector_store, chunk_size, max_docs_loaded, start_time, co, previous_run_time)\n",
        "\n",
        "    metadata_list.clear()\n",
        "\n",
        "    if (limit > 0 and max_docs_loaded >= limit):\n",
        "      break\n",
        "\n",
        "  total_time = (time.time() - start_time) + previous_run_time\n",
        "  print (\"succesfully executed workload for {0} entries with total time {1}\"\n",
        "    .format(max_docs_loaded, total_time))"
      ],
      "metadata": {
        "id": "4fMQ5XIl-Rf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_cohere_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVaZrmaYyCfR",
        "outputId": "ebe1a9ef-dcbd-417c-b71d-03bad0a666c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/Cohere___parquet/Cohere--wikipedia-22-12-en-embeddings-735980cfcb568494/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution function\n",
        "load_execute_workload(dataset)"
      ],
      "metadata": {
        "id": "s8e9cRaYx7-M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}